<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>The Voice in your Machine</title>
    <link rel="stylesheet" href="voice.css">
  </head>
  <style media="screen">

  p {
    text-indent: 50px;
  }

  </style>
  <body>
    <!-- <h1>Siri is not helpful in a snowstorm</h1>
    <br>
    <p>
      I was in the middle-of-nowhere Pennsylvania in my hatchback Honda Civic, driving New York to Chicago back from a Thanksgiving holiday weekend, on a highway that had been been cleared of snow so recently you could see the treadmarks from the plow. I was driving alone, and having had the foresight to plan my entertainment (but not to avoid the storm) I thought &#34;I hate looking at my screen while driving, especially through snow, and I&#39;ve heard Siri is a pretty good at taking voice commands for stuff like that. I&#39;ll use that on the drive back to control my music and podcasts, and see what it&#39;s all about.&#34;
    </p>
    <br>
    <p>
      I then went through a scenario that might seem familiar to many of you readers who have used any of the bevy of virtual assistants that have been reaching stores, with perhaps slight . I waited until I would soon need to make my first gas refueling stop, out in New York state on the way to the middle of Pennsylvania.
    </p>
    <br>
    <blockquote>
      &#34;Siri, how far is the closest gas station?&#34;
    </blockquote>
    <br>
    <blockquote>
      &#34;Searching &#39;closest gas station&#39; in Google.&#34;
    </blockquote>
    <br>
    <blockquote>
      &#34;No no, Siri, where is the closest gas station?&#34;
    </blockquote>
    <br>
    <blockquote>
      &#34;Searching, &#39;Closet gasket shove&#39; in Google.&#34;
    </blockquote>
    <br>
    <p>
      I went for an alternative approach. I figured that if it opened with the search in Apple maps, I&#39;d only need to look down for a second. And since Apple maps was proprietary, Siri could probably control it. Right? Still not the safest, but better than having to type something out while traveling at 58mph. And, I was getting lower and lower on gas.
    </p>
    <br>
    <blockquote>
      &#34;Siri, open gas station in maps, please.&#34;
    </blockquote>
    <br>
    <blockquote>
      &#34;Opening &#39;station maps&#39; in Google.&#34;
    </blockquote>
    <br>
    <p>
      I added the please, figuring it couldn&#39;t hurt. After all, when I swore at Siri (admit it, it was one of the first things you tried too), she pretended to get miffed with me. If she could get her feelings hurt, i thought, maybe a &#34;please&#34; would help her feel good about herself, so she would try extra hard to solve my gas station issue.
    </p>
    <br>
    <blockquote>
      &#34;Siri, search &#39;gas stations&#39; in Apple Maps&#34;
    </blockquote>
    <br>
    <blockquote>
      &#34;Opening Maps&#34;
    </blockquote>
    <br>
    <p>
      My god we were getting somewhere! I remained excited for about eight to ten seconds while i waited for a safe opportunity to take a peek at the screen. A car blew by me and I finally saw no one anywhere ahead or behind me, and took a look. Nothing about gas stations of any kind, just Apple&#39;s maps app open to my last search (Dunkin Donuts in the Loop by Chicago avenue). It only opened the app, meaning I would still need to type in the search myself. Instead, I looked out the window and waited until I saw a highway sign that had several clear gas station logos on it, pulled off, and filled up.
    </p>
    <br>
    <p>
      I couldn&#39;t stop thinking about this performance, and it inspired a serious interest in voice technology. I am a linguist by education and software programmer by trade, so obviously this subject hit my sweet spot. I followed the voice market, and as much of the technology behind it, all to try and understand why voice felt fundamentally behind other technologies. Almost any app, website, and consumer technology is fairly easy to use and has an overall consistent at performing it&#39;s function. Apple, Google, Nest, and even Ikea have ensured that this is an expectation of our modern products, not a perk.
    </p>
    <br>
    <p>
      And yet, voice technology did not feel like this, at all, but rather clunky and difficult, like an early beta. It worked, just not well and not consistently. Then, a consumer revolution in voice technology happened, and I watched as the highly-polished Google Home, Amazon Alexa, and Echo Dot reached the market. Suddenly, voice-based virtual assistants were everywhere, and I was bemused by how little the voice technology had improved. These AI Assistants use incredibly smart Artificial Intelligences, requiring whole warehouses filled with endless rows of servers stacks, all dedicated to interpreting if whether you mumbled &#34;Play Michael Buble&#34; or &#34;Play Michael Bolton&#34; at your Alexa while you also had a forkful of pie from the fridge in your mouth.
    </p>
    <br>
    <p>
      But this influx of voice-based consumer products has lead to a myriad of examples like the one I gave above, which reveal a startling truth - voice technology just isn’t very good. Siri, Alex, Google Echo make lots of mistakes and it is absolutely expected that the users to pick up the slack. It is expected that it might take a few tried for the device to understand your commands correctly, a User Experience which would be suspect or outright banned from most products meant for consumer release, either digital or physical. Can you imagine a light switch which only flipped on or off two out of three times? Or using a SaaS app like Trello and seeing the perfect feeling drag-and-drop work only one-half the time? It would be very noticeable, and probably gather fairly sharp criticism. But, three of the biggest tech companies in the world, Apple, Amazon, and Google can&#39;t come up with a really good voice assistant that can pass this criteria. This, of course, makes us ask ourselves to think of an interesting question. Why is it so hard to make good voice technology?
    </p>
    <br>
    <p>
      The answer, surprisingly enough, is not software-based, it&#39;s linguistic. Language itself, the kind you, me, and every person on the planet who participates in any sort of conversation, speaking, or listening, is an extraordinarily complicated feat that our brain performs, and the science of language, Linguistics, has had a difficult time understanding how the brain physically performs language on anything other than a piece-by-piece basis (which is difficult because several parts of the brain can often times cover for one function), nor have we been able to discover an underlying structure or architecture that would describe an overall picture of how a thought becomes speech, or how audible sounds become understood words and meanings. An architectural model (like software architecture, or a schematic for how to run a factory) would theoretically give us a model upon which we can base or machine&#39;s voice software. We can&#39;t make our computers talk smart, because we don&#39;t even understand how human beings do it, and so we fundamentally don&#39;t understand how to make computers speak and converse as well as us.
    </p>
    <br>
    <p>
      You think you know how language works, I know, you really do. It feels so simple, I think we all believe a little bit that we pretty much get it. You might not understand every single nut and bolt of a phoneme or syntactic element (or even what those are), but you&#39;re a good conversationalist who’s talented at sifting through talk for subtext and hidden meaning, so of course you understand how it is you understand, at all, what&#39;s being said. You don&#39;t even need to be able to diagram a sentence in order to talk to your buddy (Authour&#39;s note: I got a D on this in junior high, and can now speak half a dozen languages).
    </p>
    <br>
    <p>
      You talk, I listen. You say &#34;I ate an al pastor burrito yesterday&#34; and I know the event happened in the past, and there was a burrito involved. Never mind the fancy stuff, you get how talking works. But what did your brain actually do? Did the words flash before your eyes, with a brief definition of each highlighted below? Or did you picture, smell, or even taste the last burrito you ate, maintaining that feeling for 1/1000 of a second? Probably neither, but I bet it was closer to the second than the first, and probably a little bit of both.
    </p>
    <br>
    <p>
      The problem for our computer engineers who are building the next generation of voice technology is how amazingly smart our brains are, and how finely-tuned they are for linguistic tasks. When you can see the brain activity of someone speaking, in both hemispheres of the brain a lot of things seem to happen very fast all over. So many areas of our brain light up when we speak, or listen to a speech, that it&#39;s pretty hard to map out what&#39;s going on at all, which is one of the reasons we&#39;ve been unable to create a comprehensive model of how the brain perform language. Scientists have even discovered recently that your motor cortex is involved with speech [1], meaning that the same pathways used to create muscle memory (for example, when an athlete is training for a sport), get involved when we engage a friend conversationally.
    </p>
    <br>
    <p>
      Turns out, when we talk to each other, what we do in our heads is actually very, very complex to the point where we don&#39;t fully understand, or even in a few subfields partially understand, how it works. And this has slowed the progress of voice technology considerably over the past several decades, where predictions far outpaced the advancements that actually ocurred. At this point, conversational voice technology is almost like the flying car or jet pack as something we were &#34;promised&#34; to have by this point from movies/TV like 2001 and The Jetsons, and our lack of understanding of our own brains might be partly to blame. Which is a shame, because current voice products are achieving a lot of success in the marketplace proving that there is indeed a consumer desire for cool voice-based technology.
    </p>
    <br>
    <p>
      The problem with this technology, the kind of technology which you may use on one of your digital devices or which I attempted during my road trip, is that it feels so smart and so dumb at the same time. I secretly thinks this frustrates us, as we just want it to be good goddmammit. And, if it’s going to be dumb, at least have the courtesy to seem dumb. Unfortunately “dumb” is not much of an option in our current market (especially if you’re a core piece of Apple’s operating system, like Siri). After all, it’s not like I could write so thoroughly about why “the wheel” is stupid. We only expect a wheel to do one thing - roll, and to only work with an outside force (like gravity or a push). We don’t expect it to understand us when we yell at it, nor expect anything for wheels to do anything else for us like, say, making us breakfast. Unfortunately, we want our voice assistants to sound smart, understand us well, and perform tasks for us that we don’t even know we need yet, while the actual voice technology fails to reach our (probably overly-elevated) expectations.
    </p>
    <br>
    <p>
      This issue has become highly visible in recent years, as voice technology has made a huge jump to the mainstream by having improved vastly from the phone-system rats nest it was stuck in for decades, when that particular piece of technology began to appear in thousands of corporate/organizations in the 80’s and 90’s. This system was favored by utility and cable companies everywhere who wanted to cut budgets by cutting employees, and so switched to voice decision-trees which would endlessly cycle customers through the same set of options. Now, voice is in many, many digital devices, online platforms, and even the photo Kiosk at Walgreens, to the point at which most people have used voice technology in some way or another.
    </p>
    <br>
    <p>
      Every iPhone user on the planet has tried Siri, even if only by accident when holding the home button too long and accidentally saying something. Siri might be the most frustrating talking robot out there, as she is great at only occasionally fulfilling your request correctly, always giving the user a sliver of hope that she might choose to do so again in the future. Not likely. Additionally, she has one annoying habit seen quite clearly from my Siri experience,  which is to google anything it doesn’t understand. Thus, when yelling at your personal device, it will happily look up the more creative swear-words you might scream.
    </p>
    <br>
    <p>
      The Amazon Echo, with it’s Alexa AI, and the Google Home now offer competing generalized home-based digital assistant controlled entirely through voice. And they’re both moving millions of units as we speak, and were sold out for months during the holidays. And yet, the two week retention rate for Alexa apps is only 3% [2]. And if you’re like me, if 97% of the apps you download on a platform are so bad that you need to delete them after a week, you’ll eventually stop downloading apps from that platform altogether. While still relatively niche, voice has definitely made it to the mainstream in the technologies we use, warts and all.
    </p>
    <br>
    <p>
      We are going through a voice technology revolution, but I argue that we need to keep our expectations tempered as to how good at speaking we should expect our robots to get. I will speak about the reasons why human being, with all of our endearing linguistic tendencies, make life difficult for the robots who want to understand and speak like us. Following that, we will dive into aspects specific to computers and software that make language such a difficult subject to master. Then, getting into the subject of Neurology and language, we will discover why unearthing fully-realized neurolinguistic models has been so tough for modern linguists, while other biological aspects of language have progressed incredibly quickly. Finally, I’ll talk about the future of voice technology and what businesses, industries, and sciences may be the drivers of future progress, as well as making some speculations as to what we might expect to come.
    </p>
    <mbp:pagebreak/>
    <h1>The Linguistic Brain</h1>
    <br>
    <p>
      The human brain has evolved to perform language for millions of years, and maybe even tens of millions, resulting in the fine-tuned linguistic machine we all carry around in our heads. I’m going to use this section to explain that we have amazingly flexible language centers in our brain, which have created specific linguistic tendencies in our speech that our machines have a very difficult time conversing with us. I also want to talk about our aversion to fake-but-almost-real conversation partners, why this is the verbal version of uncanny valley effect, and how this may just be the result of some very specific evolutionary adaptations of our evolutionary ancestors. Finally, I argue that the ephemeral idea of “conversation” ties all of these pieces together, and is what makes language so characteristically human and difficult for our robots.
    </p>
    <br>
    <p>
      Our “Linguistic Brains” are very flexible and adaptive, meaning that you will adjust to strange language situations much faster than you might think. For example, adults actually can learn to speak another language completely fluently as if you were a native, though most people think otherwise; the CIA trains people to do it all the time. Our brain can use a lot of different resources to process language, and seems to come up with creative solutions to allow you to adapt to a lot of situations, such as adapting to strange accents and regionalisms quite well in our native language, even ones we’ve never heard.
    </p>
    <br>
    <p>
      My first time in Scotland I was delighted by how little I could understand of what was ostensibly my native language, and on the first day I was given a sympathetic overview of the local vocabulary of modern Edinburgh slang by a used-record store clerk. He seemed very hungover, but was nevertheless very good-natured. On this first day I could barely understand a word, but by the third day I got almost everything. My brain just naturally adapted to the accent, while I physically did nothing more strenuous than walk around the Edinburgh Festival drinking beer and watching street performers.
    </p>
    <br>
    <p>
      We can also understand non-native speakers of our languages quite well, even when they are far from perfect speakers. Some my most interesting conversations were when either I or the other speaker was speaking in a non-native language. In fact, I spoke Spanish exclusively with a Swedish friend while living in Madrid, and we had long, philosophical conversations about everything imaginable late into the Madrileno night, both of a speaking in a foreign language.  And, I marvel at the amount of tedious, incoherent conversations I’m thrust into in my own language, with other native speakers and, often times, demonstrating that it’s often the content, not the language ability, that makes a conversation interesting. Our ability to adapt to a myriad of linguistic situation has, however, given us some linguistic tendencies that complete boggle the computers.
    </p>
    <br> -->
  </body>
</html>
