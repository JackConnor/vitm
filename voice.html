<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>The Voice in your Machine</title>
    <link rel="stylesheet" href="voice.css">
  </head>
  <style media="screen">

  p {
    text-indent: 50px;
  }

  </style>
  <body>
    <br>
    <h1>
      Siri is not helpful in a snowstorm
    </h1>
    <br>
    <p>
      I was in the middle-of-nowhere Pennsylvania in my hatchback Honda Civic, driving New York to Chicago back from a Thanksgiving holiday weekend, on a highway that had been been cleared of snow so recently you could see the treadmarks from the plow. I was driving alone, and having had the foresight to plan my entertainment (but not to avoid the storm) I thought &#34;I hate looking at my screen while driving, especially through snow, and I&#39;ve heard Siri is a pretty good at taking voice commands for stuff like that. I&#39;ll use that on the drive back to control my music and podcasts, and see what it&#39;s all about.&#34;
    </p><br>
    <p>
      I then went through a scenario that might seem familiar to many of you readers who have used any of the bevy of virtual assistants that have been reaching stores, with perhaps slight . I waited until I would soon need to make my first gas refueling stop, out in New York state on the way to the middle of Pennsylvania.
    </p><br>
    <blockquote>
      &#34;Siri, how far is the closest gas station?&#34;
    </blockquote><br>
    <blockquote>
      &#34;Searching &#39;closest gas station&#39; in Bing.&#34;
    </blockquote><br>
    <blockquote>
      &#34;No no, Siri, where is the closest gas station?&#34;
    </blockquote><br>
    <blockquote >
      &#34;Searching, &#39;Closet gasket shove&#39; in Bing.&#34;
    </blockquote><br>
    <p>
      Obviously, I was an amateur, and didn&#39;t know exactly what I was doing wrong, so I decided to try an alternative approach. If I opened Apple maps first, then asked Siri the question, then ideally I could get the maps app to do the search, and just start giving me directions . And since Apple maps was proprietary, Siri could probably control it. Right? Still not the safest, but better than having to type something out while traveling at 58mph. And, I was getting lower and lower on gas.
    </p><br>
    <blockquote>
      &#34;Siri, open gas station in maps, please.&#34;
    </blockquote><br>
    <blockquote>
      &#34;Opening &#39;station maps&#39; in Bing&#39;.&#34;
    </blockquote><br>
    <p>
      I added the please, figuring it couldn&#39;t hurt. After all, when I swore at Siri (admit it, it was one of the first things you tried too), she pretended to get miffed with me. If she could get her feelings hurt, i thought, maybe a &#34;please&#34; would help her feel good about herself, so she would try extra hard to solve my gas station issue.
    </p><br>
    <blockquote>
      &#34;Siri, search &#39;gas stations&#39; in Apple Maps&#34;
    </blockquote><br>
    <blockquote>
      &#34;Opening Maps&#34;
    </blockquote><br>
    <p>
      My god we were getting somewhere! I remained excited for about eight to ten seconds while i waited for a safe opportunity to take a peek at the screen. A car blew by me and I finally saw no one anywhere ahead or behind me, and took a look. Nothing about gas stations of any kind, just Apple&#39;s maps app open to my last search (Dunkin Donuts in the Loop by Chicago avenue). It only opened the app, meaning I would still need to type in the search myself.
    </p><br>
    <p>
      &#34;Screw this&#34;, I said. I kept an eye out to the side of the road, and waited until I saw a highway sign that had several clear gas station logos on it, pulled off, and filled up.
    </p><br>
    <p>
      I could not stop thinking about this performance, and it inspired a serious interest in voice technology. I am a linguist by education and software programmer by trade, so obviously this subject hit my sweet spot. I followed the voice market, and as much of the technology behind it, all to try and understand why voice felt fundamentally behind other technologies. Modern technology is obsessed with user experience, with an incredible amount of energy being spent to make modern consumer products intuitive, safe, and enjoyable. Brands like Apple, Google, Nest, and Ikea have ensured that consumers expect this of our modern products, and that these features are not a perk.
    </p><br>
    <p>
      And yet, voice technology does not feel like this, at all, but rather is clunky and difficult, like something that is still being worked out by the mabufacturer. It works, just not all that well and not all that consistently. Nonetheless, a consumer revolution in voice technology is occurring, and as recently the highly-polished Google Home, Amazon Alexa, and Echo Dot have started showing up in households all over the world. Suddenly, voice-based virtual assistants were everywhere, and I am bemused by how little the voice technology has improved. These AI Assistants use incredibly smart Artificial Intelligences, requiring whole warehouses filled with endless rows of servers stacks, all dedicated to interpreting if whether you mumbled &#34;Play Michael Buble&#34; or &#34;Play Michael Bolton&#34; at your Alexa while you also had a forkful of pie from the fridge in your mouth. Yet, they struggle to get tasks like this consistently right.
    </p><br>
    <p>
      This influx of voice-based consumer products has lead to a myriad of examples like the one I gave above, which reveal a startling truth - voice technology just isn&#39;t very good. Siri, Alexa, and Google Echo make lots of mistakes and it is absolutely expected that the users to pick up the slack. It&#39;s normal if the machine takes a few tries to understand your commands correctly, a user experience consistency problem which would bar most products, either digital or physical, from a  consumer release. Can you imagine a light switch which only flipped on or off two out of three times? Or using a SaaS app like Trello and seeing the perfect feeling drag-and-drop work only one-half the time? It would be very noticeable, and probably gather fairly sharp criticism. But, three of the biggest tech companies in the world, Apple, Amazon, and Google can&#39;t come up with a really good voice assistant that can pass this criteria. This, of course, makes us ask ourselves to think of an interesting question. Why is it so hard to make good voice technology?
    </p><br>
    <p>
      The answer, surprisingly enough, is not software-based, it is linguistic. Language itself is an extraordinarily complicated feat that our brain perform, the kind of linguistic tasks performed by you, me, and every person on the planet who participates in any sort of conversation. And Linguistics, the science of language, has had a difficult time understanding how the brain physically performs language on anything other than a simplified or incomplete piece-by-piece basis, nor have we been able to discover an underlying structure or architecture that would describe an overall picture of how a thought becomes speech, or how audible sounds become understood words and meanings. An architectural model (like software architecture, or a schematic for how to run a factory) would theoretically give us a model upon which we can base or machine&#39;s voice software. We can&#39;t make our computers talk smart, because we don&#39;t even understand how human beings do it, and so we just don&#39;t know how to make computers speak and converse as well as us human beings.
    </p><br>
    <p>
      This may be surprising that our best and brightest struggle to understand the nature of language. After all, you probably think you know how language works. It feels so simple, I think we all believe a little bit that we pretty much get it. You might not understand every single nut and bolt of a phoneme or syntactic element (or even what those are), but you&#39;re a good conversationalist who&#39;s talented at sifting through talk for subtext and hidden meaning, so of course you understand how it is you understand, at all, what&#39;s being said. You don&#39;t even need to be able to diagram a sentence in order to talk to your buddy (Authour&#39;s note: I got a D in this subject in junior high, and can now speak half a dozen languages). You talk, I listen. You say &#34;I ate an al pastor burrito yesterday&#34; and I know the event happened in the past, and there was a burrito involved. Never mind the fancy stuff, you get how talking works.
    </p><br>
    <p>
      But what did your brain actually do? Did the words flash before your eyes, with a brief definition of each highlighted below, forming a nice easy sentence for you to read? Or did you picture, smell, or even taste the last burrito you ate, maintaining that feeling for 1/1000 of a second? Probably neither, but I bet your linguistic experience was closer to the second than the first, and there was probably a little bit of both involved. It&#39;s not uncommon to unconsciously repeat, visualize, or even write down new words when you hear them (that&#39;s your brain trying to reinforce the knowledge), but often times memories are triggered by words, potentially conveying complex sensory memories (such as the smell of the taco stand) which also provide important meaning to those words.
    </p><br>
    <p>
      The problem for our computer engineers who are building the next generation of voice technology is how amazingly smart our brains are, and how finely-tuned they are for linguistic tasks. When you can see the brain activity of someone speaking, in both hemispheres of the brain a lot of things seem to happen very fast all over. So many areas of our brain light up when we speak, or listen to a speech, that it&#39;s pretty hard to map out what&#39;s going on at all, which is one of the reasons we&#39;ve been unable to create a comprehensive model of how the brain perform language. Scientists have even discovered recently that your motor cortex is involved with speech [1], meaning that the same pathways used to create muscle memory (for example, when an athlete is training for a sport), get involved when we engage a friend conversationally.
    </p><br>
    <p>
      Turns out, when we talk to each other, what we do in our heads is actually very, very complex to the point where we don&#39;t fully understand, or even in a few subfields partially understand, how it works. And this has slowed the progress of voice technology considerably over the past several decades, where predictions far outpaced the advancements that actually occurred. At this point, conversational voice technology is almost like the flying car or jet pack as something we were &#34;promised&#34; to have by this point from movies/TV like 2001 and The Jetsons, and our lack of understanding of our own brains might be partly to blame. Which is a shame, because current voice products are achieving a lot of success in the marketplace proving that there is indeed a consumer desire for cool voice-based technology.
    </p><br>
    <p>
      The problem with this technology, the kind of technology which you may use on one of your digital devices or which I attempted to use during my road trip, is that it feels so smart and so dumb at the same time. I secretly thinks this frustrates us, as we just want it to be good and, if it&#39;s going to be dumb, at least have the courtesy to seem dumb. Unfortunately &#34;dumb&#34; is not much of an option in our current market (especially if you&#39;re a core piece of Apple&#39;s operating system, like Siri). After all, it&#39;s not like I could write so thoroughly about why &#34;the wheel&#34; is stupid. We only expect a wheel to do one thing - roll, and to only work with an outside force (like gravity or a push). We don&#39;t expect it to understand us when we yell at it, nor expect anything for wheels to do anything else for us like, say, making us breakfast. Unfortunately, we want our voice assistants to sound smart, understand us well, and perform tasks for us that we don&#39;t even know we need yet, while the actual voice technology fails to reach our (probably overly-elevated) expectations.
    </p><br>
    <p>
      This issue has become highly visible in recent years, as voice technology has made a huge jump to the mainstream by having improved vastly from the phone-system rats nest it was stuck in for decades, when that particular piece of technology began to appear in thousands of corporate/organizations in the 80&#39;s and 90&#39;s. This system was favored by utility and cable companies everywhere who wanted to cut budgets by cutting employees, and so switched to voice decision-trees which would endlessly cycle customers through the same set of options. Now, voice is in many, many digital devices, online platforms, and even the photo Kiosk at Walgreens, to the point at which most people have used voice technology in some way or another.
    </p><br>
    <p>
      Every iPhone user on the planet has tried Siri, even if only by accident when holding the home button too long and accidentally saying something. Siri might be the most frustrating talking robot out there, as she is great at only occasionally fulfilling your request correctly, always giving the user a sliver of hope that she might choose to do so again in the future. Not likely. Additionally, she has one annoying habit seen quite clearly from my Siri experience,  which is to just search anything it doesn&#39;t understand. Thus, when yelling at your personal device, it will happily research the more creative swear-words you might scream.
    </p><br>
    <p>
      The Amazon Echo, with it&#39;s Alexa AI, and the Google Home now offer competing generalized home-based digital assistant controlled entirely through voice. And they&#39;re both moving millions of units as we speak, and were sold out for months during the holidays. And yet, the two week retention rate for Alexa apps is only 3% [2]. And if you&#39;re like me, if 97% of the apps you download on a platform are so bad that you need to delete them after a week, you&#39;ll eventually stop downloading apps from that platform altogether. While still relatively niche, voice has definitely made it to the mainstream in the technologies we use, warts and all.
    </p><br>
    <p>
      We are going through a voice technology revolution, but I argue that we need to keep our expectations tempered as to how good at speaking we should expect our robots to get. I will get into the reasons why human being, with all of our endearing linguistic tendencies, make life difficult for the robots who want to understand and speak like us. Following that, we will dive into aspects specific to computers and software that make language such a difficult subject to master. Then, getting into the subject of Neurology and language, we will discover why unearthing fully-realized neurolinguistic models has been so tough for modern linguists, while other biological aspects of language have progressed incredibly quickly. Finally, I&#39;ll talk about the future of voice technology and what businesses, industries, and sciences may be the drivers of future progress, as well as making some speculations as to what we might expect to come.
    </p><br>
    <mbp:pagebreak />
    <br>
    <h1>
      The Linguistic Brain
    </h1>
    <br>
    <p>
      The human brain has evolved to perform language for millions of years, and maybe even tens of millions, resulting in the fine-tuned linguistic machine we all carry around in our heads. However, these amazingly flexible language centers in our brain have created specific tendencies in our speech that cause our digital machines to have a difficult time conversing with us. I will also talk about our aversion to fake-but-almost-real conversation partners, why this is the verbal version of uncanny valley effect, and how this may just be the result of some very specific evolutionary adaptations of our evolutionary ancestors. Finally, I argue that the ephemeral idea of &#34;conversation&#34; ties all of these pieces together, and is what makes language so characteristically human and difficult for our robots.
    </p><br>
    <p>
      Our &#34;Linguistic Brains&#34;, by which I mean the systems in the brain that work together to allow us to converse, are very flexible and adaptive, meaning that you as a normal human being will adjust to strange language situations much faster than you might think. For example, despite rumors to the contrary, adults actually can learn to speak another language completely fluently; after all the CIA trains people to do it all the time. Our brain can use a lot of different resources to process language, and seems to come up with creative solutions to manage new linguistic situations, such as adapting to strong accents or picking up regionalisms without even realizing it, even ones we&#39;ve never previously heard.
    </p><br>
    <p>
      My first time in Scotland I was delighted by how little I could understand of what was ostensibly my native language, and on the first day I was given a sympathetic overview of the local vocabulary of modern Edinburgh slang by a used-record store clerk who was very hungover. After he gave me some tips on Scottish slang I remember thinking &#34;Is this even English? I can&#39;t understand a word being said.&#34; On this first day I could barely understand a word, but by the third day I got almost everything said to me by even the most drunken of Scottish revellers. My brain just naturally adapted to the accent, while I physically did nothing more strenuous than walk around the Edinburgh Festival drinking beer and watching street performers.
    </p><br>
    <p>
      We can also understand non-native speakers of our languages quite well, even when they are far from perfect speakers. Some my most interesting conversations were when either I or the other speaker was speaking in a non-native language. In fact, I spoke Spanish exclusively with a Swedish friend while living in Madrid, and we had long, philosophical conversations about everything imaginable late into the Madrileno night, both of a speaking in a foreign language.  And I marvel at how often I&#39;m thrust into incoherent, tedious conversations in my own native language with other native speakers, demonstrating that it&#39;s the content, not the language ability, that makes a conversation interesting. Our ability to adapt to a myriad of linguistic situation has, however, given us some linguistic tendencies that complete boggle the computers.
    </p><br>
    <p>
      Tone-of-voice, word order, and word choice are a nightmare for computers, because us humans just love to play both with our words and sentences as we speak them. Tone is an especially pesky one. Raising your voice at the end of a sentence may be one of the trickiest examples of this. You raise your voice to make a question, right? But what if you&#39;re Australian or, especially, from New Zealander, then you might raise your voice at the end of a normal sentence as part of your speech pattern. If you&#39;re from the San Fernando Valley, or parts of the Pacific Northwest, then you might also end a lot of sentences that way, all day, everyday. It&#39;s just part of your accent. But how is a computer supposed to know this, especially if you are a relatively new user?
    </p><br>
    <p>
      This inflection, called a High Rising Terminal, is a much more generalized part of speech (more like a sound, like &#34;tthhh&#34; or &#34;wwww&#34;) then many English speakers might think. Many, many languages do not use an HRT inflection to make a sentence a question. In fact, tonal languages literally cannot change the tone or inflection of their speech like this, at all. In Mandarin, for example, It would completely change the meaning of the words. Even the basic &#34;Nin hao ma&#34; (how are you?) could be interpreted as &#34;You&#39;re good, mammoth&#34; if the last syllable is given that high-rising lilt at the end, and heard as a fourth tone. And, in fact, English could survive very well without the HRT as well. After all, with no punctuation or HRT you could still easily discern most questions from statements, as I&#39;m willing to bet you understand the fundamental difference between:
    </p><br>
    <blockquote>
      &#34;We are going to the store to buy ice cream&#34;
    </blockquote><br>
    <p>
      And,
    </p><br>
    <blockquote>
      &#34;Are we going to the store to buy ice cream&#34;
    </blockquote><br>
    <p>
      I argue that that, without the inflection (or the punctuation, even), we&#39;d all pretty much understand what is and is not a question through word choice alone. We keep the High Rising Terminal because we like it, it&#39;s kind of fun, and until we stop using this part of speech it will remain a confounding vex for our computers when we do voice. This is an example of a nuance that nature has prepared us humans to be very good at picking out, but which is hard to build software to manage. After all, does the software engineer write a true/false statement every time Alexa hears that uptick to ask &#34;Ok, is he from New Zealand? What about Studio City?&#34; It&#39;s not impossible, a subprogram like this could be written. But it&#39;s not easy, and getting this right with someone the first time your computer ever talks to a user is pretty much an educated guess.
    </p><br>
    <p>
      I use English in a lot of examples in this book, which is both my native language and one I find really fun, with one very interesting fact. This fact may actually be impossible to prove, but according to my Phonology Professor at the University of Barcelona, English has more nouns than any other language on the planet. Apparently, we Anglophones have a thing for specifics and categorization, and so we are happy to have seventeen words for &#34;cup&#34;, when fewer synonyms suffice in almost every other language.
    </p><br>
    <p>
      There are many interesting reasons for this, the first being that Modern English is the bastard love-child of German and French when William the Conquerer crashed his way into England in 1066. Everyone then started to simplify the verb tenses from Anglo-Norman (the pre-cursor to French), and the noun cases in German, while assimilating about ten or fifteen thousand French words into English, with a staggering number of vowels. Initially through the Church and stationed military in England that act as enforcement, but eventually through the Upper Class Aristocracy. This class structure can be even be seen in many of our duplicate words, just think of why we need a word for both &#34;beef&#34; (from French) and &#34;steak&#34; (German). This merging period lasted until they fully bled together to form &#34;Middle English&#34;. Middle English is also the closest a modern speaker might have any chance of speaking or understanding, if they were to be sent back in time. If a tardis lands in your backyard, don&#39;t go back further than about 1300, you really won&#39;t understand what anyone is saying, even in Britain.
    </p><br>
    <p>
      And this fact sucks for computers. Having lots of nouns is confusing, especially when things like age of the talker, slang, and regional upbringing can all affect the words you or I might choose. Hell, I change the words I might say depending how I feel in that exact moment. Sometimes I&#39;ll even try out a new phrase I heard earlier in the dat, or a word I learned on a podcast, without event thinking about it, just naturally flexing my linguistic muscles. Even a super smart AI like Alexa or Deep Blue might simply not have the contextual clues to figure out that &#34;Alexa, find me a Pot place nearby&#34; means I&#39;m looking for kitchen utensils, as the algorithm is simply making an educated guess.
    </p><br>
    <p>
      Not that English is uniquely complex, we just happen to have a lot of nouns, and every language has proven a challenge to translate to computers. Romance languages, for example, have are tricky because of their precise use of time via conjugation patterns. This can drive computers bonkers, trying to differentiate between &#34;leo&#34; (he read past simple tense) and &#34;leo&#34; (I read present tense), both technically 100% correct sentence  (subject + indirect verb). As spanish does not require the use of a subject pronoun (in English this would be &#39;I, you, she/he, we, they&#39;, to name a few), a conjugated verb can sometimes be an entire sentence, expressing a subject, verb, and potentially an object. &#34;!Traemelo!&#34; (bring it over here, Spanish), is a frequent one heard at the rowdier tables at the bar or, &#34;Andartene!&#34; (got out of here!, Italian), what an Italian bouncer might yell while kicking you out of a club. All single words, all complete sentences with a lot of implied meaning through their structure.
    </p><br>
    <br>
    <h3>
      The Uncanny Valley
    </h3>
    <br>
    <p>
      Another problem for the engineers creating voice technology is that we demand realism, and are relentlessly determined to root out a fake. It&#39;s the same part of the brain that refuses to allow us to see CGI faces and think they look real, falling into the Uncanny Valley. As a result, we actually have a linguistic Uncanny Valley, triggered by things that seem close to, but not exactly, human.
    </p><br>
    <p>
      The Uncanny Valley, coined by Japanese roboticist Masahiro Mori, is the idea that there is a peak of realism for animated or computer generated characters in TVs and movies. If a character is on the &#34;not-so-realistic&#34; side of this peak, like Barney, C3PO, or any of the Simpsons, viewers will accept the character fairly easily.
    </p><br>
    <p>
      However, if one creates characters that are too realistic, falling to the &#34;hyperrealistic&#34; side of this peak, and into the Uncanney Valley. Hyperrealistic digital characters, such as Tom Hanks character in the The Polar Express or the Rock in Mummy: Curse of the Scorpion King, are rejected strongly by audiences, and it is often talked about as a major negative of any of these movies. We hate uncanny valley characters, they just don&#39;t sit right with us when, oddly, we would be much more comfortable with a much less realistic portrayal.
    </p><br>
    <p>
      There is also a verbal Uncanny Valley as well, as we can sometimes get easily frustrated or dislike fake voices, and particularly interacting with them. I personally get annoyed every single time I call a utility and need to speak and listen to a decision tree, and have learned to go immediately to keypad mode. What&#39;s interesting is that interaction is often the cause of this, as almost anybody will listen to a pre-recorded message over an intercom at a bus station without thinking, but the second they need to get a robot to understand what they want, massive frustration sets in.
    </p><br>
    <p>
      This X-men like ability to distinguish real voices from fake is not easy on our algorithms, either, because as much as they&#39;ve tried they can&#39;t seem to get past this feature of human intelligence. The famous Turing Test is misunderstood as having proposed the idea of an ultimate Robot test: to have a conversation with a human where the human didn&#39;t know that it was a machine with whom they were conversing. But this test has been tried many times with many different voice technologies, with no one getting close. The best AI of our time cannot fool most of us for even a second, our brains are too hard-wired to recognize the difference. Fans of Malcolm Gladwell will recognize this as a &#34;blink reflex&#34; [3], a snap judgement you make almost instantaneously and without conscious interference. It also says that our technology will need to get very good in order to overcome our natural tendencies.
    </p><br>
    <p>
      In an attempt to learn to mimic humankind&#39;s linguistic ability, we&#39;ve built computer algorithms to listen to thousand of billions of snippets of human language, all to teach itself usig enormous computing power, how to talk like us. The results are the products (Siri, Alexa) we&#39;ve heard about here. What we&#39;ve learned, really, is that it&#39;s very hard to mimic human speech to such a degree that actual humans don&#39;t notice the difference. Very, very hard, apparently because our ears and brains have adapted incredibly well to recognizing what is and is not an actual human voice. This may be an instinct learned long ago, and buried deep within our genetic heritage.
    </p><br>
    <p>
      I have an evolutionary biology streak, though I&#39;m not always proud of it as it often leaves one wildly speculating as to the source of genetic traits, almost none of which is in any way provable. Guessing the reasons for specific adaptive traits is literally impossible unless you have a time machine to take your GoPro back to the Eoarchean era to see which specific creatures were hunting and eating our ancestors.
    </p><br>
    <p>
      When you look at our specific trait of intuitively understanding with great accuracy whether or not a voice actually comes from a real human, however, it does make you think about what kind of predators might have chased us back then. There are many animals now that can mimic our voices, the parrot most famously, so perhaps something really big, with sharp teeth and claws, would hide in a cave. It might&#39;ve then lied in wait, making a noise that sounded a lot like the Proto-Indoeuropean for &#34;help help&#34;, casually eating off the members of Homo Sapien which did not have a strong aptitude for distinguishing real human voices from a fakel. If so, perhaps we still carry this ability with us today, a remnant of our more precarious past.
    </p><br>
    <br>
    <h3>
      The Conversation Dilemma
    </h3>
    <br>
    <p>
      We&#39;ve got to get the conversation side of speech better if we want our robots to converse on our human level, as right now we&#39;re mastering the mechanics of voice technology, but not the understanding. As the great Roger Ebert always said about bad noir films, we &#34;play the notes, but not the music&#34;. I could be wrong, it&#39;s hard to tell with such lapses in our knowledge about voice technology, but I believe that we need to master creating machines which understand the content of a conversation, not just the words. There are many ways we drive these improvements, including Machine Learning techniques derived from those discussed later, improved linguistic modelling, a general industry improvement to foster better technology, and even via a general growth in innovation.
    </p><br>
    <p>
      It&#39;s unknown at this time if Artificially Intelligent algorithms that use Machine Learning to improve their language skills could ever flawlessly mimic a human being. I&#39;m not so sure, personally. I mean, who knows? Maybe if you feed millions and millions of conversations and speeches and accents into a big huge mainframe, it can learn to mimic a human. But maybe there&#39;s some ephemeral je-ne-sais-quoi to language that a computer could never really perfectly imitate. For the same reason that an artificial flavor could never really taste exactly like a fresh picked strawberry, I don&#39;t know that a computer could truly pull of speaking in the voice of, say, my East Coast Grandmother, without me knowing the difference. In both cases, almost anyone could tell the difference every single time.
    </p><br>
    <p>
      If the conversation level is at, say,an C3PO-level assistant, that I&#39;m all in for in the next 20 years or so, maybe less. Honestly,C3PO almost seems like the Amazon&#39;s Echo&#39;s British boarding school cousin, who is a few years older and wiser, though he was never famous for being intelligent (that was R2D2). Our Machines are pretty good, too bad they have such a hard time holding the thread of a conversation. Perfection will be very difficult for Machine learning in this field, but ML does have the possibility of helping us make some incredible advances in the field, and in fact already has. But even basic conversational skills has been elusive in artificial intelligence, so there are no guarantees how fast or slow voice technology may advance in our lifetimes.
    </p><br>
    <p>
      Us humans have mastered a linguistic system with many aspects that can be very tricky for computers. But, computers and software (including Machine Learning and Artificial Intelligence) themselves also have systemic attributes that make language difficult. In the next section, we will look at the issue from the computer point-of-view.
    </p><br>
    <mbp:pagebreak />
    <br>
    <h1>
      How Computers Talk
    </h1>
    <br>
    <p>
      When I think of the perfect, human-like, talking robot, I think of Priss from Blade Runner. One of my all-time favorite favorite movies, Blade Runner is a about five humanoid robots called replicants who can walk, talk, and apparently even perform a strip-tease with a snake, who escape from slave-labor off-world colony and go hide out on Earth where they are outlawed. The premise is &#34;If a machine looks like us and acts like us, then why what&#39;s the difference between this machine and a human?&#34;, which becomes a very real situation for Harrison Ford as the police-contracted bounty hunter who is supposed to hunt down and kill these robots (a job position called a &#34;Blade Runner&#34;). We see the ostensible &#34;bad guy&#34; robots mourn, laugh, philosophize, and ultimately the leader commits a very unselfish act. This is in contrast to the human characters, who are generally seen as cogs in gigantic bureaucracies, whether the police or the monolithic corporation that builds the newest, Nexus-6 replicants, and have resigned themselves to staying on a dying earth that almost everyone else has abandoned for off-world colonies.
    </p><br>
    <p>
      Based on a novel by Phillip K. Dick, the undisputed sci-fi champion of questioning the nature of reality and existence, it forces the viewer to ask themselves &#34;What does it mean to be alive?&#34;. It also presents the question &#34;Could a thing we create deserve &#39;rights&#39;?&#34;, an issue which Bill Gates brought to the forefront recently, when he proposed that robots should pay taxes. It also had an all-time great Harrison Ford performance, Daryl Hannah in her prime, Director Ridley Scott just after the first the first Alien, and a score that was so influential it created it&#39;s own sub-genre of music. If you have not seen this movie, you should.
    </p><br>
    <p>
      Blade Runner is an exceptional film, which gives us robots that speak and listen so much like humans that the protagonist (Harrison Ford) must administer an emotional intelligence test to know for sure whether he&#39;s talking to a human or a replicant. It even opens,
    </p><br>
    <blockquote>
      &#34;The Nexus 6 replicants were superiour in strength and agility, and at least equal in intelligence, to the genetic engineers who created them.&#34;
    </blockquote><br>
    <p>
      Blade Runner has proven prophetic in terms of the technologies it presents, but when it comes to the speaking and conversational abilities of the Nexus-6, it could&#39;ve been jetpacks to the moon, as we&#39;ve made only a fraction of the progress shown on the screen. What we&#39;ve come to find in the real world is that strength and agility are no problem, replacing human physical tasks with robot labor in the real world has largely been conquered, as much factory and farm work has almost entirely replaced human workers with automated machines. Boston Dynamics has released several videos (which quickly went viral), of their swimming, crawling, bat-flapping robots which mimic animal and human movement to perform amazing tasks.
    </p><br>
    <p>
      The problem is when it comes to &#34;intelligence&#34;, the machines are way behind what we &#34;expected&#34; by looking at thsi same pop culture. While physical jobs get rapidly automated out of existence by robots that work tirelessly at peak performance (think about all those assembly line jobs, and where they went), when it comes to intelligent machines that can talk, converse, and generally perform higher-level decision making, the results are not quite so stunning. In fact, we are stuck with the  Siri and Alexa which, while amazing pieces of technological achievement, come nowhere close to the expectations that have been matched and succeeded in the physical realm. Voice technology, much like Marty McFly&#39;s Hoverboard from back to the future, remains struggling in the shallow end of the technological swimming pool.
    </p><br>
    <p>
      The machines we build and which run our software have such a hard time with voice, despite a lot of optimism in from both our scientists and our pop-culture, for quite a few reasons inherent to their very nature. I first want to break apart the different types of voice that our machines actually perform, showing how they are actually quite good in two out of the three major fields in voice technology, but that the last crucial one, conversationalism, is the current Achilles heel, the weak point in humanity&#39;s collective voice technology. I then will guide us through the current state of voice software, examining how Machine Learning is creating artificially intelligent programs which might be able to bypass our current lack of linguistic knowledge. After, I&#39;ll go over some of the  history of AI and voice technology and wrap up the section by showing how the evolution of voice as a consumer product has led to its current weaknesses and strengths.
    </p><br>
    <br>
    <h3>
      Robot Voices
    </h3>
    <br>
    <p>
      In the world of voice technology, there are three major subfields that can be generalized; two of these our computer software and hardware is quite good at, but the third subfield, conversationalism, we still struggle. Though specialists might (rightly) disagree with my broad categorization, I want to make the subject clear to you, the reader, and so have split the field of robot-voice into phonetic (sounds), syntactic (grammar), and semantics (content and meaning) skills. The first we are already very good at, with Machine Learning providing many of the advances in the field. The second, grammar, we are already pretty good at as our best voice tech can generally make and understand grammatical sentences. The third, &#34;conversationalism&#34;, or the ability to hold a decent conversation (sometimes thought of as the Turing Test), we are extremely bad at. And, once again, we&#39;ll find out how our inability as humans to understand how we speak has been a serious annoyance when trying to create a semantic device which can actually hold a conversation with us.
    </p><br>
    <p>
      The first of the voice-technology sub-fields is phonetic voice interpretation, otherwise known as &#34;getting the user&#39;s individual words right when they speak&#34;. This sub-section of NLP is the science that allows a sentence you say to show up, word-for-word, written in your text message when you&#39;re attempting not to look at your phone while replying to a text message at 55 miles per hour. When this is done well, even sentences as similar as &#34;I saw a gnat&#34; and &#34;I saw an ant&#34; can be differentiated by the computer first try, every try, by any speaker with any level of mumbliness.
    </p><br>
    <p>
      Much of this is new, as there have been serious advances in the field recently. In 2017 for the first time a language program (Dragon) scored a lower average error rate than the professional typists taking dictation. Though this side of NLP is fascinating in it&#39;s own right, this is not the aspect of language technology that we will be talking about. I may write about this in a later piece, it&#39;s a really fun subject, but for now just know that our computers can understand the words you speak very, very well and in the future, much more so. Expect voice-identification and transcription to get incredibly good in the next five to ten years.
    </p><br>
    <p>
      The second voice-tech sub-field is understanding, transcribing, and forming sentences of a grammatical nature.This is a robot that can understand not just sounds, but is able to identify who the speaker might be, or which person was giving what to whom; it also includes translation tools that translate the words in one language and turn them into an intelligible sentence in another language. Anyone who speaks another language knows, direct translation is a great way to sound totally insane in another language. So, the computers also needs to understand the grammar behind translated sentences well enough that they can present them to a native speaker the second language, and not just some kind of word-for-word reply.
    </p><br>
    <p>
      Text replies and translations created by an algorithm have traditionally suffered grammatically, but they are getting better rapidly thanks largely to some of these Machine Learning advances, since now the big AI programs get to practice millions of times a day from real-world conversation snippets. This is necessary, as each sentence in almost any language has many permutations and versions, all of which the computer must be able to parse and understand. Thanks to big data, our machine learning algorithms actually get to practice most of these, and so are getting better by brute-force, teaching themselves all the rules by example. Unfortunately, our natural grammar patterns in every language include a truckload of exception, called &#34;irregular rules&#34;.
    </p><br>
    <p>
      Make no mistake, irregular rules are also the bane of every Ancient Greek student as well, because they are common in every single language known to man, and by definition they don&#39;t fit the patterns followed by other similar grammatical structures.  These are almost always shortcuts, colloquialisms, or vestigial leftovers created for heavily used word and language patterns. Much like school kids will wear a path by cutting through a front yard, we love to shorten and modify our most heavily used words to make them easy to say. No one outside a renaissance faire says &#34;Ten of the Clock&#34;, we all say &#34;Ten O&#39;Clock&#34;. Not to mention, &#34;inorganic&#34; means &#34;not organic&#34;, but &#34;inflammable&#34; actually means &#34;extremely flammable&#34;, as shown hilariously in The Simpsons. Not a fun mistake to make, and a hard meaning for either a human or a computer to intuit on our own.
    </p><br>
    <p>
      It&#39;s not just the irregular grammar rules that throw off the computer from building a perfectly grammatical sentence, it&#39;s also the endless permutations of each grammatical structure we allow, and how each has a slightly different meaning to us, even if we don&#39;t consciously realize it. &#34;Take out the trash, would you?&#34; has a more menacing tone than &#34;Would you take out the trash?&#34; Simple word switches can massively alter the meaning of simple sentences, confounding computers algorithms, which are learning as fast as they can. But, it&#39;s slow work.
    </p><br>
    <p>
      Luckily, the latest voice tech is getting unimaginably close to having consistently grammatical sentences, and though in the next 20 years robots may never fully speak a perfectly native-accent version of our languages (though it might), the advances have been interesting and allowed us to learn quite a lot.
    </p><br>
    <p>
      The third, and most difficult aspect of voice technology is to understand the semantic content and participate in the flow of a conversation. If done excellently, this would allow your computer to understand what you say, and to respond to your questions or comments intelligently, or even humanly. This subfield has experienced high expectations and low payoffs, largely because we have underestimated it&#39;s difficulty, and also partly because we have been unable to understand and reverse engineer how the human brain performs speech. Our machine learning has also struggled to understand how non-linguistic content (everything from intonation to the scent of pheremones) affect how we speak and interpret language.
    </p><br>
    <mbp:pagebreak/>
    <br>
    <h1>
      Learning machines, artificial intelligences
    </h1>
    <br>
    <p>
      We lack a coherent model for how we humans understand language, which would be fine except that our Machine Learning algorithms can&#39;t figure it out either. We really don&#39;t understand how we, ourselves, speak the way we do (see the next chapter about this subject)  so, when it comes to applying neurolinguistic models to our computer algorithms, well we don&#39;t have any neurolinguistic models, just guesses. Our machine learning has also made some advances in this field, but has struggled mightily to push forward and has come nowhere near as far along as the other two sub-topics above. A brief explanation of how modern software learns language tasks will demonstrate why this this are, semantic content, is especially difficult for machines to master, as well as what approaches we are taking to overcome this.
    </p><br>
    <p>
       The first thing to understand is that the internet fueled Big Data (by having lots and lots of data), which fueled the first wave of search and voice technology, which we find ourselves in now. The catalyst fueling this revolution is that we&#39;ve recently built computers powerful enough to carry out the millions of decisions and computations needed to be performed every 1/10 of a second, and feed them enough billions of pieces of data to take advantage of branch of a computer science called Machine Learning that, until recently, was mostly theoretical when it came to Natural Language Processing. Tapping into this new found ability has allowed us to get closer to building a computer that can actually talk and speak like a human than ever before. Unfortunately, it&#39;s also helped put into sharp focus how much further we have to go to realistically pull this off.
    </p><br>
    <p>
      That&#39;s a lot of crazy linguistic and technical words, so let me back up. Natural Language Processing is the area of technology concerned with human languages. If you search for &#34;local blackberry farms&#34;  then you are using NLP because Google or Firefox or whatever needs to interpret your string of words which you typed into the search bar into a question, and then turn that question into an actual list of search results that resembles information you are actually looking for. Or, for example, if you yell the natural language phrase &#34;Siri, closest Wendy&#39;s Drive Thru stat!&#34; and by some miracle she gets the message, she&#39;ll translate your sentence into a search term which it will then turn into longitude and latitude coordinates, that it will put onto your map in the form of a pin. These are then is then run through another set of algorithms to give you step-by-step directions, effectively translating your human question into a computer-readable question, and back into a human-centric response. Hence, the processing of natural languages.
    </p><br>
    <p>
      Machine Learning, is more like using guess-and-check to find the best method of doing something, by looking at millions of pieces of data and performing millions of guesses on them. This is the process we used to get really good at NLP, as we build algorithms which learn how to speak, self-taught by looking at millions of examples of speech and making guesses.  Let me give you an example.
    </p><br>
    <p>
      A program that the bank one day might use to verify the signature matches the name, may work perfectly now but, initially when the program was first turned on, it was quite bad at this. The program was then fed millions and millions of signatures with corresponding names that it could guess and check against (a &#34;supervised&#34; AI algorithm), measuring pixel colors, groupings, and other traits for general &#34;signature-ness&#34;, until it basically figures out what it thins are all the variations that a human&#39;s version of their name, as a signature, might look like. When it&#39;s accurate enough, the programmers will build some technology around that ability and start to put it into the ATM machines.
    </p><br>
    <p>
      Ever do a Captcha when authenticating somewhere online? This is where you need to &#34;prove you&#39;re human&#34; by typing a short word that looks like it comes from a grainy photo. Turns out, you might&#39;ve helped train one of these Artificial Intelligences, as part of Google&#39;s Computer Vision machine learning program. By writing out the word from a (seemingly) old textbook, you were providing an answer for there Artificial intelligence program to guess against.  Machine Learning is important to voice technology because this same process is used, with bits of conversations from a myriad of different sources (lately Twitter has been a popular one), to create algorithms that can interpret and (hopefully one day) intelligently respond to your conversation.
    </p><br>
    <p>
      Artificial Intelligence is the easiest to describe, but the hardest to really understand. Simply, any artificial intelligence is just a program that can learn and get better at something. This can be as simple as a program which learns to navigating a maze, teaching itself by running into walls and hitting dead ends as it tries and advances,getting better from it&#39;s mistakes. Or, the program can be made to learn lots of things from many different disciplines and genres, such as Watson, the television-friendly Aritificial  that IBM put on jeopardy for a series of shows in which it absolutely demolished the competition including all-time champion Ken Jennings. It is not just a Jeopardy program, it&#39;s also used in a huge amount of software and tools. It just also happens to be really good at Jeopardy (and Go, as it just beat all the worldwide Go champions). As we&#39;re about to see, this is a long way from AI&#39;s humble roots in academia.
    </p><br>
    <br>
    <h3>
      The old man in the cave
    </h3>
    <br>
    <p>
      Speaking robots has been in pretty much every type of science fiction since science fiction had robots at all (see the Twilight Zone episode &#34;The Old Man in the Cave&#34; for a great black-and-white example of this). As a result, it&#39;s been on pretty much every computer scientist&#39;s mind since the beginning of it&#39;s own era. Some of them thought it was easy, unfortunately, and fueled by creative energy and excitement these computer scientists, I imagine, would head to the linguistic department to ask &#34;Hey how do we humans perform language? I need to build a talking program, and I just want to program my computers to do it the way we do it&#34;, hoping for a perfectly reverse-engineerable model to cull from. And boy, were they ever let down. These enthusiastic scientists were incredibly smart and amazingly hard-working, but there was a lot they didn&#39;t understand about the limitations of computers, Machine Learning, and Neural Networks (much of which we still don&#39;t know today).
    </p><br>
    <p>
      During the first great push in Machine Learning from 1951, when Marvin Minsky created the first actual Neural Network (called the SNARC) to the late 70&#39;s, there was great energy around the idea of using computers to replicate a human brain. Many of the best Computer Scientists in the U.K. and the United States were given large amount of money by the government to make this happen, and the general consensus was that the task would be a relatively straight-forward one. Even back then they understood a ton about brain micro-workings, understanding how to replicate the dendrite-axon-chemical synaptic relationship for example, and they had been very successful at mimicking other human movements (walking, jumping) with robots and automatons. They asked themselves, How hard could it be to build a computer brain? Would it even be possible? As we know now, they were incredibly unsuccessful, as everyone involved had assumed the task would be an easy or, minimum, a possible one.
    </p><br>
    <p>
      There was then, and remains now, a lot of speculation around what an engineer could, and could not, program a computer to do. My friend Mohammad Jouni tells about one of his MIT Computer Science professors, who said she started in the seventies in Artificial Intelligence research with the goal to make a robot that could cook. Today, this is still apparently her goal, though I thought of her recently when I watched the demo of an automated kitchen that seemed to make a bolognese pretty well, with sleek quiet white flexible robot hands stirring the sauce over a hot stove, that then slid sleekly into the walls and counter-tops.
    </p><br>
    <p>
      Language technology was no more successful than cooking technology, unfortunately, and by the early 70&#39;s most of the government money had been yanked from voice-technology and artificial intelligence research, with little from the private sector to compensate. That doesn&#39;t mean there weren&#39;t noble attempts before the modern era of AI. In 1964, Daniel Bobrow wrote an algebra-word-problem solver in Lisp for his Ph.D thesis, which supposedly worked very well at solving the type of word problems you probably did in high school, such as what might happen if a train from Seattle and a train from Portland travelled towards each other at 65 miles per hour. The rest were either basic bots, using a tree-structure (like a phone answering machine tree, where it has pre-programmed responses to look for), or basic translation tools. A lot of theory was laid out regarding linguistic-capable computers, but successful prototypes were noticeably lacking until the recent wave of AI.
    </p><br>
    <p>
      Luckily for Google, research into Machine Learning didn&#39;t entirely dry up, and a lot of focus was placed into Neural Networks in the 80&#39;s, which we now use for an extraordinary amount of Artificial Intelligence uses. Research continued, mostly in Universities such as Stanford where Sergey Brin got his feet wet, while servers got more powerful, and computing cheaper and faster. Eventually, the theories that required millions of pieces of data analyzed were suddenly possible to build and feed the required data. Neural Networks have become especially important for current iterations of automated assistant voice technology, and is key to the ability of Alexa, Siri, and Google Home to understand and respond to your commands.
    </p><br>
    <br>
    <h3>
      We speak with our eyes
    </h3>
    <br>
    <p>
      The more we study humans and how we speak, the more good research has appeared about the importance of nonverbal language (gestures, eye movement, etc…). In fact, the effect is so strong that experts recommend against using sarcasm or making sarcastic jokes via text/email/etc, as these require a tone or physical indicator to demonstrate to the recipient that it is a joke. Otherwise, they just read it in there head with their normal voice. One study claims that up to 55% of our conversations are actually done via non-linguistic methods (body language [4]), though I have no idea how one might prove that. Nobody (yet) has the physical data for a Machine Learning algorithm to teach itself to understand or, even, mimic humans&#39; non-verbal conversational tactics. But, products like the Kinect and Fitbit are collecting mountains of this physical-bio data from humans worldwide, so maybe Siri will one day be able to simply understand my massive eye roll when I say &#34;play Rock&#34;  and it puts on Cold Play, and change the station without me even needing to ask.
    </p><br>
    <p>
      Our current Natural Language Processing work tends to focus almost exclusively on the content of the words. This is partially technological; user input is a basic part of computing, whereas reading biometrics or facial computer vision is advanced even now, there&#39;s a huge technological time gap between these two. It&#39;s also the programmers&#39; faults, for loving words so much. A huge part of our job involves slicing and dicing and pasting together words and sentences all day long, like some kind of demented film editor (when editors use to edit big reels of film by cutting them and carefully attaching them back together). It&#39;s far from the only task we perform in that code editor of ours, but it is a frequent one. So, when it came time to analyze and mimic language, we collectively said &#34;Words? We love analyzing words!&#34;, and got to it. When the only tool you have is a hammer, everything looks like a nail.
    </p><br>
    <p>
      One way the machines could learn our language better, however, would be to study more than just the words that are said in a conversation. It&#39;s even been speculated that it&#39;s actually impossible to do proper natural language processing without analyzing the face as the users speaks. As creepy as it may sound, it may take advanced, real-time face-analyzation (or bio-metrics, such as in the Johnny Depp vehicle Transcendence) for even a smart computer algorithm to properly converse with you. Well undoubtedly find out, as bio-sensing technology improves at a rapid clip as well, and soon this data will become available for the ML algorithms.
    </p><br>
    <mbp:pagebreak />
    <br>
    <h1>
      The Brain Voice Dilemma
    </h1>
    <br>
    <p>
      Language Models of the brain need to be understood better, both for the sake of self-understanding, and in order to create a truly great voice technology. I feel optimism in this field because we live in a time with amazing technology for scientists to deep dive into the subject. Light-based brain probes, are allowing us to do non-invasive explorations of the brain. Scientists and engineers will continue to map out in more detail the specific areas, and what specific part the play in the linguistic process.
    </p><br>
    <p>
      We&#39;ve also learned quite a lot from those who have experienced brain trauma in some form, especially stroke victims. From this we have learned two very important facts, upon which many linguistic theories of all kinds are based, both coming from observations made when a specific part of the brain stops working and language is affected.
    </p><br>
    <p>
      The first fact is, if the Wernicke&#39;s area stops working, you will keep making short, normal sounding sentences with normal sounding words, but they will be lack some or all meaning. The speaker, who suffers from Wernicke&#39;s Aphasia (from the Latin for speechlessness), will struggle to create even a few meaningful words that can be uttered together, and is more prone to speaking . Though they understand quite well, the production of language, spoken and written, becomes difficult or impossible. What we think this means is that the Wernicke&#39;s area controls linguistic/grammatic/vocabularic structure, or more simply the meaning of what you say when you turn a thought into words.
    </p><br>
    <p>
      The second fact, is that the damage to the Broca&#39;s Area causes the victim to produce normal sounding sentences made of partially or completely crazy words, even though the speaker thinks that they are talking normally. Imagine you&#39;re chasing a road runner through the desert and he/she manages to drop an anvil on your head, and you damage your Broca&#39;s area. You now have Broca&#39;s Aphasia, and you will think you are talking completely normally, but in fact it&#39;s getting to be very strange how everybody keeps asking looking confused when you talk, and you can&#39;t understand what they&#39;re saying even though clearly everybody is speaking in English. Apparently, Broca&#39;s Aphasia affects word choice, though there is lots of debate on this subject (debate being a commonality you will find among the many diverse subjects of Neurolinguistics).
    </p><br>
    <p>
      I would take this opportunity to present you with great detailed blueprint of how the Broca&#39;s and Wenicke&#39;s area&#39;s work together and as individual components, to help us, as humans perform the miracle that is speech. Really I would, in loving detail, explaining the entire process from a - z, with several pretty diagrams. But we have no clue. We don&#39;t even understand in what direction these two talk, what they are really doing when they create or interpret linguistic signals from our ears and eyes, or how they help me order my hotdog with mustard and relish. We have only the faintest sketch of what these two do at all, and almost zero insight into the language-creation process. Which, of course, is a problem for our talking robots.
    </p><br>
    <p>
      We know a lot about language, don&#39;t get me wrong. We&#39;ve mapped out a truly incredible amount of the brain, mostly using MRI&#39;s, which how all the different parts light up when different words are heard or said (this is not the only experiment of the type, a large number of truly disgusting or depraved things have occurred in MRI machines in the name of Neuroscience. As a result, we&#39;ve mapped the sections of the brain which light up form sexual activity very thoroughly). And, as a result, we think we have a good grasp of, at least, what most of parts of the brain are that get involved in speech. But how they interact together as a fluid machine to allow you to understand every word your chimney sweep says to you, is pretty much a complete mystery.
    </p><br>
    <mbp:pagebreak />
    <br>
    <h1>
      The Future of Voice
    </h1>
    <br>
    <p>
      Business drivers have always been huge for pushing technology, assuming the businesses are playing fair. Voice should be no exception, so it will be interesting to see how future businesses direct voice technology. We can even see that in the voice products today, since the phone tree answering machines your parent used to use, discussed earlier, only required a simple call-and-response, where the most important features were that the customer could understand the recorded answering machine question, and the machine could understand the customer&#39;s reply. Thus, commercial interests had us parsing words and replying to short, preset questions, which we were ultimately very successful at and whose legacy might even be part of the reason AI like Alexa is fundamentally built for commands, not conversation. Perhaps, further increase in the commercial prospects of voice technology will lead to some interesting advancements.
    </p><br>
    <p>
      That&#39;s why I get so excited when I see that there are three competing Therapy-based apps where you talk to a robot (theoretically some kind of an artificial intelligence). Not because I think AI therapy is the world&#39;s next trillion dollar business, but because it of the voice challenge presented to the developers, and the creative solutions they will need to pull off to get it to work. Therapy bots need to be conversational to work well, since a therapist needs to get you to say what you need to say on your own behalf (nothing in Freudian therapy can be forced, otherwise it is not accepted). And &#34;the art of conversation&#34; is one of our major weaknesses in voice technology right now, and to which one of these therapy bots might provide a breakthrough to improvement.
    </p><br>
    <p>
      Amazon has one of the best reasons to get it&#39;s Echo home-assistant in as many places as possible - it creates more sales through Amazon. A report [5] that came out 6 months ago states an average 6% increase in sales by Amazon Echo owners, something which Jeff Bezos and company should be remarkably excited about as with enough customers that could easily translate to billions of dollars, even if they don&#39;t make any money on the Echo devices themselves. This is certainly the reason they&#39;re aggressively pushing the $49.99 Echo Dot, even thought at that price it is almost definitely not itself a money-maker. For them, it&#39;s just another distribution platform to sell product.
    </p><br>
    <p>
      The Google Home is somewhere in the middle, as a product, because it&#39;s part of Google&#39;s new hardware-intensive series of consumer initiatives (the Pixel, as well, which include Google Voice), but it is also a valuable source of raw data for DeepMind, the AI which Google recently bought as part of an acquisition, and which it is training to compete with Microsoft&#39;s Deep Blue technically, as well as the Echo for consumers. Google is the undisputed champion of online advertising ($51.8 billion from AdWords alone in 2015), and so they also undoubtedly have their eye on a practical advertising revenue through the Google Home, though it is unclear what exactly they may plan to roll out. Much like Amazon, this would allow them to merely extend an existing revenue strategy (AdWords) through the Google Home.
    </p><br>
    <p>
      Think about when you&#39;ve gone through a voice-based versus touch-based phone system to do something as basic as paying a bill. When they go right, they both take about the same amount of time, and the voice is perhaps even a little bit faster. But, when voice goes wrong you can get stuck in endless loops, or your accent can mean a specific word or number gets interpreted differently by the voice recognition and you get directed to the wrong department where you need to go through another set of voice commands.
    </p><br>
    <p>
      It&#39;s much more reliable to use the touch version, after all you can&#39;t misinterpret hitting a keypad number. It&#39;s also much easier to error-correct, as most phone trees offer a &#34;hit nine to return to main menu&#34; option, as opposed to voice-based systems which have a tendency to get stuck in certain error patterns. Analyzing the situation economically, the upside benefit of voice is relatively minor, and the downsides can be quite large when using voice versus touch systems. Most of us do this math intuitively, and say &#34;no thanks&#34;.
    </p><br>
    <p>
      Despite this, the hands-free market is skyrocketing, though both consumers and manufacturers alike are in the early-stage of figuring out exactly what it is that they want. You can hook up WeMo, Alexa, or Google Home voice to control your Hue light bulbs, if you want to walk around changing colors, dimming, and turning them on and off over and over. In fact, I know several people who have and are happier than the lotus eaters to control everything from their couch. But I don&#39;t think this is for me, especially considering the hundreds of dollars I need to spend to get set up. But, likely much of this voice will move towards personal devices, and the appliance-connectivity will become a more established fact of life. It honestly doesn&#39;t seem all that unreasonable that in twenty years we&#39;ll assume we can talk to our toaster oven, though I don&#39;t know quite what we&#39;d talk about. &#34;Is the parmesan on the slice of bread toasted?&#34;, &#34;Yes, Jack&#34;, &#34;Thank you toaster&#34;.
    </p><br>
    <mbp:pagebreak />
    <br>
    <h1>Sources</h1>
    <br>
    <p>
      1. https://www.sciencedaily.com/releases/2017/02/170215101444.htm
    </p><br>
    <p>
      http://www.recode.net/2017/1/23/14340966/voicelabs-report-alexa-google-assistant-echo-apps-discovery-problem
    </p><br>
    <p>
      https://www.amazon.com/Blink-Power-Thinking-Without/dp/0316010669
    </p><br>
    <p>
      https://www.psychologytoday.com/blog/beyond-words/201109/is-nonverbal-communication-numbers-game
    </p><br>
    <p>
      http://www.retaildive.com/news/npd-echo-device-owners-spend-more-on-amazon/426505/
    </p><br>
  </body>
</html>
